name: Benchmarks

on:
  push:
  pull_request:
  workflow_dispatch:

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure data/results dirs
        run: mkdir -p data results

      - name: Generate trace if missing (deterministic)
        run: |
          python -c "import json, random, os; p='data/freq_wins_50k.jsonl'; \
if not os.path.exists(p): \
  random.seed(42); hot=[f'hot-{i}' for i in range(50)]; cold=[f'cold-{i}' for i in range(2000)]; \
  f=open(p,'w',encoding='utf-8'); \
  [f.write(json.dumps({'prompt': (random.choice(hot) if random.random()<0.6 else random.choice(cold)), 'response': 'response-for-'+(random.choice(hot) if random.random()<0.6 else random.choice(cold))})+'\\n') for _ in range(5000)]; \
  f.close(); \
print('trace_ready:', p)"

      - name: Sanity check files
        run: |
          ls -lah
          ls -lah data || true
          python -c "import os, itertools; p='data/freq_wins_50k.jsonl'; \
print('exists:', os.path.exists(p), 'size:', os.path.getsize(p) if os.path.exists(p) else 0); \
print('first two lines:'); \
[print('> ', line.strip()) for line in (open(p,'r',encoding='utf-8').read().splitlines()[:2] if os.path.exists(p) else [])]"

      - name: Run baseline
        run: |
          python3 bench_gptcache.py \
            --trace data/freq_wins_50k.jsonl \
            --dim 512 --capacity 102400 \
            --base-lat 0.0005 --per-token 0.0002 \
            --mode baseline

      - name: Run TinyLFU
        run: |
          python3 bench_gptcache.py \
            --trace data/freq_wins_50k.jsonl \
            --dim 512 --capacity 102400 \
            --base-lat 0.0005 --per-token 0.0002 \
            --mode tinylfu_admit

      - name: Generate plots
        run: python3 plot_results.py

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: results/**
